---
title: "JunjieZeng_STATS485_Unit3_Paper_V2"
author: "Junjie Zeng"
date: "2025-04-28"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview    
This file produces outcomes in paper Prognostic Modeling with Texas Education Data. This paper is based on the URPS project in Winter 2025 semester supervised by Prof. Ben Hansen and PhD student Julian Bernado. We used TEA_2019.csv data to model Texas students' math and reading test scores in grade 3-8 in 2019. Caroline Moy collaborated with me on this project. She was in charge of the modeling for grade 6-8 reading scores and grade 3-5 math scores. Since our paper depends on long-running computations, we will show the modeling process only for grade 5 reading scores as the representatives using 30 percent schools in this file. Other models can be built using basically the same way. This file is divided into several parts: 

1. Data preprocessing
2. Identify Potential important variable
3. Build simple models
4. Select the best simple model
5. Normalization
6. Splines
7. Best Model for Grade 5 Reading
8. Extremely Low Math Scores  

# Part 0: Data Loading    
Reproduciability
```{R}
set.seed(489)
```

We load the data and necessary packages. 
```{r}
library(readr)
library(dplyr)
library(lme4)
library(purrr)
library(ggplot2)
library(stringr)
library(splines)
data <- read_csv("/home/rstudio/TEA_2019.csv")
```

# Part 1: Data preprocessing       
In this part, we use two functions `create_dataset` and `clean_dataset` to help us create datasets for each grade and subject. 
There will be 12 different datasets named as df_grade_subject, e.g. df_5_r is the dataset we are using in this file. 
We clean our data follow the following steps: 
1. We randomly sample 30 percent of schools to run faster. 
2. Remove all variables with only NA values. Those variables contains no information. 
3. We also take out alternative test scores, they are not being modeled for now. If we are modeling primary school students, we also take out all variables ends with `_midd`. `replacement_id` and `acadyear` are also irrelevant. 
4. If we are modeling reading scores, we take out all math related variables, vice versa. 
5. We add `age_int` variable, which is the rounded version of `age`. We will treat age as categorical variable rather than continuous variable. Similarly for `attend_p0` and `attend_m1`, percentage of attendance present year/last year. 

```{r}
#' Function to sample and clean our data
#'
#' @param data The TEA data
#' @param sample Proportion of schools to sample, default 0.3
#' @return A cleaned data with school sampled
create_dataset <- function(data, sample = 0.3){
  set.seed(489)
  unique_schools <- unique(data$schoolid_nces_enroll_p0)
  schools_sampled <- sample(unique_schools, 
                            size = round(sample * length(unique_schools)))
  
  df <- data %>% 
    filter(schoolid_nces_enroll_p0 %in% schools_sampled) %>% 
    # Remove variables with only NA's
    select(where(~ !all(is.na(.)))) %>% 
    mutate(age_int = round(age), 
           attend_p0_d1 = round(attend_p0, 1),
           attend_m1_d1 = round(attend_m1, 1))
           
  return(df)
}

#' Function to create datasets for each grade and subject
#'
#' @param data The cleaned data
#' @return A list of datasets for each grade and subject of the form df_grade_subject, e.g. df_5_r
clean_dataset <- function(data){
  dfs <- list()
  for(grade in 3:8){
    for(subject in c("m", "r")){
      cols_to_drop_all <- c('glmath_alt_scr_m1', 'glmath_alt_scr_p0',
                            'readng_alt_scr_m1', 'readng_alt_scr_p0',
                            'replacement_id', 'acadyear')
      cols_to_drop_subject <- if (subject == "r") {
        c('glmath_ver_p0', 'glmath_lan_p0', 'glmath_scr_p0')
      } else {
        c('readng_ver_p0', 'readng_lan_p0', 'readng_scr_p0')
      }
      dfs[[paste("df", grade, subject, sep="_")]] <- data %>% 
        select(where(~ !all(is.na(.)))) %>% 
        select(-any_of(cols_to_drop_all)) %>% 
        select(-any_of(cols_to_drop_subject)) %>% 
        select(
          if (grade < 6) {
            any_of(names(data)[!stringr::str_ends(names(data), "_midd")])
          } else {
            everything()
          }
        )%>% 
        filter(gradelevel == grade)
    }
  }
  return(dfs)
}
```

```{r}
df <- create_dataset(data)
dfs <- clean_dataset(df)
```

# Part 2: Identify Potential important variable   
We select categorical variables with number of categories less than 15 and create summary values for them to see whether there's a relatively big difference between categories. Here we only show the list for grade 5 reading. 
```{R}
#' Function to get numerical summary of one categorical variable from data
#'
#' @param var Categorical variable
#' @param data The TEA data
#' @return A dataframe containing numerical summary of one categorical variable from data
get_var_summary <- function(var, data1){
  data1 %>% 
    group_by(!!sym(var)) %>% 
    summarize(mean(readng_scr_p0, na.rm = T), 
              median(readng_scr_p0, na.rm = T),
              count = n(),
              proportion = n()/nrow(data1))
}

#' Function to get numerical summary of categorical variables from data
#'
#' @param df data frame
#' @return numerical summaries of categorical variables less than 15 categories from data
get_var_summary_list <- function(df){
  vars <- names(df)[sapply(df, function(x) length(unique(x)) < 15)]
  summary_list <- map(vars, ~get_var_summary(var = .x, data1 = df))
  return(summary_list)
}
```
```{R}
get_var_summary_list(dfs[["df_5_r"]])
```
From the summaries, it seem every category variable in the list has some impact on reading scores. What about continuous variable reading score from last year? 
```{r}
ggplot(dfs[["df_5_r"]], aes(x = readng_scr_m1, y = readng_scr_p0)) +
  geom_hex(bins = 50) +                       
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(title = "Current vs Past score (smoothed)")
```
We see past year reading score and present year reading score are nonlinearly related. This also indicates the usage of polynomial terms/spines. 

# Part 3: First Stage Best Subset Selection
Last time we used pseudo forward selection. This time we apply a different methods. 
1. We first decide some fixed variables that we want to include in every model: readng_scr_m1/glmath_scr_m1, race, gender. 
   Preliminary study has shown previous year test score is a strong predictor of present year score. And race and gender are two natural variables people concern. 

2. Now we categories the variables into different groups. Variables in a same group measure the same thing, but may be vary in space or time. 
For example, (enrfay_school, enrfay_state, enrfay_district) is one group, (frl_now, frl_2yrs, frl_ever,...) is another group. We select one representative from each group because they overlap greatly and thus we believe the existence of strong multicollinearity within each group. But we do keep the lagged variables. For example we keep both "persist_inferred_m1" and "persist_inferred_p0". The selection of representatives can be investigated further in future, but for now, here's the list of representatives: 

"enrfay_school"
"frl_ever"
"lep_ever"
"migrant_ever"
"homeless_ever"
"specialed_ever"
"persist_inferred_m1"       
"persist_inferred_p0"      
"transferred_out_m1"        
"transferred_out_p0"       
"chronic_absentee_m1"       
"chronic_absentee_p0"
"readng_lan_m1"            
"attend_p0_d1"              
"attend_m1_d1"

3. We again divided representatives into two groups: group 1 and group 2. We will build model for all combination of variables in group 1. Group 2 contains all the lagged variables. 

Group 1:
"enrfay_school"
"frl_ever"
"lep_ever"
"migrant_ever"
"homeless_ever"
"specialed_ever"
"persist_inferred_p0"      
"transferred_out_p0"       
"chronic_absentee_p0"
"attend_p0_d1"              

Group 2:
"persist_inferred_m1" 
"transferred_out_m1"   
"chronic_absentee_m1"
"readng_lan_m1" 
"attend_m1_d1"

4. We build simple models for all combination of variables in group 1. 
Here we only show models for grade 5 reading. 
A general formula would be:  reading_score_past_year ~ fixed variables + a combination of group 1 variables + school random effect 

```{R message=FALSE, warning=FALSE}
#' Function to create a list to keep lists of models
#'
#' @param
#' @return a list of lists named `c_mod_grade_subject` for future use
create_candidate_mod_list <- function(){
  c_mod_list <- list()
  for(grade in 3:8){
    for(subject in c("m", "r")){
      c_mod_list[[paste("c_mod", grade, subject, sep="_")]] <- list()
    }
  }
  return(c_mod_list)
} 

#' Function to create formulas for modeling
#'
#' @param subject `r` for reading, `m` for math
#' @param predictors predictors to be written in formula
#' @return a formula to be passed in `lmer` function
create_formula <- function(subject, predictors){
  if(!(subject %in% c("m", "r"))){
    stop("Subject should be m or r")
  }
  if(subject == "m"){
    response <- "glmath_scr_p0"
  } else {
    response <- "readng_scr_p0"
  }
  fixed_part <- paste(predictors, collapse = " + ")
  formula <- as.formula(paste0(response, " ~ ", fixed_part, " + (1 | schoolid_nces_enroll_p0)"))
  return(formula)
}

#' Function to generate candidate models for a given subject and grade
#'
#' @param candidate_vars candidate variables whose combination will be modeled
#' @param fixed_vars fixed variables that will be included in every model
#' @param subject `r` for reading, `m` for math
#' @param grade an integer from 3-8
#' @param list1 a list to save models, default `c_mod_list`
#' @return candidate models passed into the `c_mod_grade_subject` list in the `c_mod_list`
create_candidate_mod <- function(candidate_vars, fixed_vars, subject, grade, list1) {
  all_combinations <- map(0:length(candidate_vars), ~ combn(candidate_vars, m = .x, simplify = FALSE)) |> flatten()
  
  formulas <- map(all_combinations, ~ create_formula(subject = subject, predictors = c(fixed_vars, .x)))
  
  list1[[paste("c_mod", grade, subject, sep="_")]] <- 
    map(formulas, ~ lmer(.x, data = dfs[[paste("df", grade, subject, sep = "_")]], REML = FALSE))
  
  return(list1)
}

```
```{r message=FALSE, warning=FALSE}
candidate_vars_1 <- c("frl_ever",
                    "lep_ever",
                    "attend_p0_d1",
                    "specialed_ever",
                    "transferred_out_p0",
                    "enrfay_school",
                    "chronic_absentee_p0",
                    "homeless_ever",
                    "persist_inferred_p0",
                    "migrant_ever")

fixed_vars_1 <- c("readng_scr_m1",
                "gender",
                "raceth")
c_mod_list <- create_candidate_mod_list()
create_candidate_mod(candidate_vars_1, fixed_vars_1, "r", 5)
```

Now we have 2^10 - 1, which is 1023 models. We want to choose the best model. The best model is chosen by AIC/BIC/MSE values. However, those three criteria do not always agree on the best model. As a compromise, we take the mean of their rank, and select the best model. 

```{R}
#' Function to generate candidate models for a given subject and grade
#'
#' @param candidate_vars candidate variables whose combination will be modeled
#' @param fixed_vars fixed variables that will be included in every model
#' @param subject `r` for reading, `m` for math
#' @param grade an integer from 3-8
#' @return candidate models passed into the `c_mod_grade_subject` list in the `c_mod_list`
model_selection <- function(grade, subject, list1){
  list_name <- paste("c_mod", grade, subject, sep="_")
  aic_values <- map_dbl(list1[[list_name]], AIC)
  bic_values <- map_dbl(list1[[list_name]], BIC)
  mse_values <- map_dbl(list1[[list_name]], function(model) {
    preds <- predict(model)
    truth <- model@frame[[1]]
    mean((preds - truth)^2)
  })

# Combine everything nicely into a tibble
  comparison_table <- tibble(
    model_id = seq_along(list1[[list_name]]),
    AIC = aic_values,
    BIC = bic_values,
    MSE = mse_values
  )

  print(comparison_table)
  
  comparison_table <- comparison_table %>%
    arrange(MSE) %>%
    mutate(mse_rank = row_number()) %>% 
    slice(1:50) %>% 
    mutate(
      rank_aic = rank(AIC, ties.method = "first"),
      rank_bic = rank(BIC, ties.method = "first"),
      mean_rank = (rank_aic + rank_bic) / 2
    ) %>% 
    arrange(mean_rank)
  
  return(comparison_table)
}
```
```{r}
model_selection(5, "r",c_mod_list)
```
The comparison table tells us that model 849 is the best model. The formula for it is readng_scr_p0 ~ readng_scr_m1 + gender + raceth + 
frl_ever +  lep_ever + attend_p0_d1 + specialed_ever + transferred_out_p0 + enrfay_school + chronic_absentee_p0 + (1 | schoolid_nces_enroll_p0)

# Part 4: Second Stage Best Subset Selection
Now we have the best simple model from fixed variables and group 1. We now treat the variables in the best simple model as the fixed variables, and so a best subset selection over group 2. 
Fixed variables: 
readng_scr_m1 + gender + raceth + frl_ever +  lep_ever + attend_p0_d1 + specialed_ever + transferred_out_p0 + enrfay_school + chronic_absentee_p0
Group 2:
"persist_inferred_m1" 
"transferred_out_m1"   
"chronic_absentee_m1"
"readng_lan_m1" 
"attend_m1_d1"
```{r message=FALSE, warning=FALSE}
candidate_vars_2 <- c("persist_inferred_m1",
                      "transferred_out_m1",
                      "chronic_absentee_m1",
                      "readng_lan_m1", 
                      "attend_m1_d1")

fixed_vars_2 <- c("readng_scr_m1",
                  "gender",
                  "raceth",
                  "frl_ever",
                  "lep_ever",
                  "attend_p0_d1",
                  "specialed_ever",
                  "transferred_out_p0",
                  "enrfay_school",
                  "chronic_absentee_p0")
c_mod_list_2 <- create_candidate_mod_list()
c_mod_list_2 <- create_candidate_mod(candidate_vars_2, fixed_vars_2, "r", 5, c_mod_list_2)
```
```{r}
model_selection(5, "r", c_mod_list_2)
```
We see the best model is model 14. And its formula is 
readng_scr_m1 + gender + raceth + frl_ever + lep_ever + attend_p0_d1 + specialed_ever + transferred_out_p0 +  enrfay_school + chronic_absentee_p0 + chronic_absentee_m1 +  readng_lan_m1


# Part 5: Normalization   
In this part, we investigate whether normalization will help with our model. We will use the winner model as the control. 
```{R warning=FALSE}
unscaled <- lmer(readng_scr_p0 ~ readng_scr_m1 + gender + raceth + frl_ever + lep_ever
                 + attend_p0_d1 + specialed_ever + transferred_out_p0 
                 +  enrfay_school + chronic_absentee_p0 
                 + chronic_absentee_m1 +  readng_lan_m1
                 + (1 | schoolid_nces_enroll_p0), 
                 data = dfs[["df_5_r"]], REML = FALSE)

scaled <- lmer(scale(readng_scr_p0) ~ scale(readng_scr_m1) + gender + raceth + frl_ever + lep_ever
                 + attend_p0_d1 + specialed_ever + transferred_out_p0 
                 +  enrfay_school + chronic_absentee_p0 
                 + chronic_absentee_m1 +  readng_lan_m1
                 + (1 | schoolid_nces_enroll_p0), 
                 data = dfs[["df_5_r"]], REML = FALSE)

cat("\nThe AIC for unnormalized model is", AIC(unscaled))
cat("\nThe AIC for normalized model is", AIC(scaled))
cat("\nThe BIC for unnormalized model is", BIC(unscaled))
cat("\nThe BIC for normalized model is", BIC(scaled))
```

We see normalization results in a better model. So we update our best model to the normalized one. 

# Part 6: Splines   
Now we explore the use of spline for our model. 
`mod0` is without splines, `modi` is has spline of degree i. 
We create 11 models, first one without any splines, the rest natural splines 1-10.
```{R}
splines <- list() 
splines[['mod0']] <- lmer(scale(readng_scr_p0) ~ scale(readng_scr_m1) 
                          + gender + raceth + frl_ever + lep_ever
                          + attend_p0_d1 + specialed_ever + transferred_out_p0 
                          +  enrfay_school + chronic_absentee_p0 
                          + chronic_absentee_m1 +  readng_lan_m1
                          + (1 | schoolid_nces_enroll_p0), 
                          data = dfs[["df_5_r"]], REML = FALSE)
for(i in 1:10){
  splines[[paste('mod', i)]] <- lmer(scale(readng_scr_p0) ~ ns(scale(readng_scr_m1),i) 
                                      + gender + raceth + frl_ever + lep_ever
                                      + attend_p0_d1 + specialed_ever + transferred_out_p0 
                                      +  enrfay_school + chronic_absentee_p0 
                                      + chronic_absentee_m1 +  readng_lan_m1
                                      + (1 | schoolid_nces_enroll_p0), 
                                      data = dfs[["df_5_r"]], REML = FALSE)
}
```

Now like we did before, we get AIC/BIC/MSEs for each mdoel.   
```{R}
splines_Sum <- list()

for(i in seq_along(splines)) {
  splines_Sum[[i]] <- list(
    model = splines[[i]],
    AIC = AIC(splines[[i]]),
    BIC = BIC(splines[[i]]),
    MSE = mean(residuals(splines[[i]])^2)
  )
}
names(splines_Sum) <- names(splines)

aic_values <- sapply(splines_Sum, function(x) x$AIC)
aic_values

bic_values <- sapply(splines_Sum, function(x) x$BIC)
bic_values

mse_values <- sapply(splines_Sum, function(x) x$MSE)
mse_values
```

We create line plots to visualize how AIC/BIC/MSE change with the increase of spline degrees.     
```{R}
aic_df <- data.frame(
  Model = names(aic_values),
  AIC = aic_values
)
aic_df <- subset(aic_df, Model != "mod0")
aic_df$Order <- as.numeric(str_extract(aic_df$Model, "\\d+"))
aic_df <- aic_df[order(aic_df$Order), ]
aic_df$Model <- factor(aic_df$Model, levels = aic_df$Model)

ggplot(aic_df, aes(x = Model, y = AIC, group = 1)) +
  geom_line() +
  geom_point() +
  labs(title = "AIC Across Models", x = "Model", y = "AIC") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{R}
bic_df <- data.frame(
  Model = names(bic_values),
  BIC = bic_values
)
bic_df <- subset(bic_df, Model != "mod0")
bic_df$Order <- as.numeric(str_extract(bic_df$Model, "\\d+"))
bic_df <- bic_df[order(bic_df$Order), ]
bic_df$Model <- factor(bic_df$Model, levels = bic_df$Model)
ggplot(bic_df, aes(x = Model, y = BIC, group = 1)) +
  geom_line() +
  geom_point() +
  labs(title = "BIC Across Models", x = "Model", y = "BIC") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{R}
mse_df <- data.frame(
  Model = names(mse_values),
  MSE = mse_values
)
mse_df <- subset(mse_df, Model != "mod0")
mse_df$Order <- as.numeric(str_extract(mse_df$Model, "\\d+"))
mse_df <- mse_df[order(mse_df$Order), ]
mse_df$Model <- factor(mse_df$Model, levels = mse_df$Model)

ggplot(mse_df, aes(x = Model, y = MSE, group = 1)) +
  geom_line() +
  geom_point() +
  labs(title = "MSE Across Models", x = "Model", y = "MSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```  


# Part 7: Best Model for Grade 5 Reading    
So based on our analysis, the best model we have for grade 5 reading is 
scale(readng_scr_p0) ~ ns(scale(readng_scr_m1),3) 
                      + gender + raceth + frl_ever + lep_ever
                      + attend_p0_d1 + specialed_ever + transferred_out_p0 
                      +  enrfay_school + chronic_absentee_p0 
                      + chronic_absentee_m1 +  readng_lan_m1
                      + (1 | schoolid_nces_enroll_p0)


                      
# Part 8: Extremely Low Math Scores   
One worth-noting pattern in our data is extremely low math scores for students in grade 6-8. We have lots of students scored 1043, which we can reasonably guess corresponding to raw score 0 in STAAR math test based on past grading schemes (Grading scheme for 2019 was not found online). 
Here's a histogram show the pattern. 
Grade 6 math score distribution: 
```{R}
data_sampled <- data %>% 
  filter(schoolid_nces_enroll_p0 %in% schools_sampled)  %>% 
  select(gradelevel, glmath_scr_p0) %>% 
  na.omit()
for(i in 3:8){
  p <- data_sampled %>%
    filter(gradelevel == i) %>%
    ggplot(aes(x = glmath_scr_p0)) +
    geom_histogram(bins = 100) +
    labs(
      title = paste("Grade", i, "math score distribution"),
      x = "Score",
      y = "Count"
    ) +
    theme_minimal()
  
  print(p)
}
```